{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the Data from Flat Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('output_list.txt', sep=\" \", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('InputFile.csv', header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the Data from Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('File.xlsx', sheetname='Sheet1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the Data from Data Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From MySQL\n",
    "import MySQLdb \n",
    "# be sure to change the host IP address, username, password and database name to match your own\n",
    "connection = MySQLdb.connect (host = \"192.168.1.2\", user = \"user\", passwd = \"password,\n",
    "                              db = \"scripting_mysql\")\n",
    "# prepare a cursor object using cursor() method\n",
    "cursor = connection.cursor ()\n",
    "# execute the SQL query using execute() method.\n",
    "cursor.execute (\"select name_first, name_last from address\")\n",
    "# fetch all of the rows from the query\n",
    "data = cursor.fetchall ()\n",
    "# close the cursor object\n",
    "cursor.close ()\n",
    "# close the connection\n",
    "connection.close ()                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From SQL Server\n",
    "import pypyodbc \n",
    "connection = pypyodbc.connect('Driver={SQL Server};'\n",
    "                                'Server=localhost;'\n",
    "                                'Database=testdb;'\n",
    "                                'uid=user;pwd=password') \n",
    "cursor = connection.cursor() \n",
    "SQLCommand = (\"SELECT customerid, firstname, lastname, city FROM customers\") \n",
    "cursor.execute(SQLCommand) \n",
    "results = cursor.fetchone() \n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From Oracle\n",
    "import cx_Oracle\n",
    "db = cx_Oracle.connect('hr', 'hrpwd', 'localhost:1521/XE')\n",
    "cursor = db.cursor()\n",
    "cursor.execute('SELECT * FROM jobs')\n",
    "# fetch all of the rows from the query\n",
    "data = cursor.fetchall ()\n",
    "# close the cursor object\n",
    "cursor.close ()\n",
    "# close the connection\n",
    "connection.close ()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the Data From Twitter"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Web Scrapping for Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "###Need to copy and paste the URL of twitter account you wish\n",
    "url =u'https://twitter.com/sachin_rt'\n",
    "r=requests.get(url)\n",
    "soup=BeautifulSoup(r.text,\"html.parser\")\n",
    "tweets=[i.p.get_text().lower().encode('ascii', 'ignore').strip() \n",
    "        for i in soup.find_all('li', {\"data-item-type\":\"tweet\"})]\n",
    "###Latest Tweet\n",
    "tweets[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Using Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "##Be sure to change the crendentials below to match your own\n",
    "consumer_key = 'YOUR-CONSUMER-KEY'\n",
    "consumer_secret = 'YOUR-CONSUMER-SECRET'\n",
    "access_token = 'YOUR-ACCESS-TOKEN'\n",
    "access_secret = 'YOUR-ACCESS-SECRET'\n",
    " \n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    " \n",
    "api = tweepy.API(auth)\n",
    "\n",
    "for tweet in tweepy.Cursor(api.user_timeline).items(10):\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for status in tweepy.Cursor(api.home_timeline).items(10):\n",
    "    # Process a single status\n",
    "    print(status.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the Data from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wikipedia as wp\n",
    " \n",
    "#Get the html source\n",
    "html = wp.page(\"List of U.S. states by Hispanic and Latino population\").html().encode(\"UTF-8\")\n",
    "df = pd.read_html(html)[0]\n",
    "df.to_csv('beautifulsoup_pandas.csv',header=0,index=False)\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the Data from any URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "url = \"http://analyticstraining.com\"\n",
    "request = urllib.request.Request(url)\n",
    "page = urllib.request.urlopen(request)\n",
    "soup = BeautifulSoup(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.p.get_text().lower().encode('ascii', 'ignore').strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Data from Big Data environement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pydoop.hdfs as hdfs\n",
    "with hdfs.open('/user/myuser/filename') as f:\n",
    "    for line in f:\n",
    "        do_something(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed import Executor,hdfs,  progress\n",
    "##Change According to your needs\n",
    "e = Executor('IPAddress:Port')\n",
    "e\n",
    "data = hdfs.read_csv('/nyctaxi/2014/*.csv',\n",
    "               parse_dates=['pickup_datetime', 'dropoff_datetime'],\n",
    "               skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyhs2\n",
    "import pandas as pd;\n",
    "\n",
    "query = \"\"\"select * from crosssell where siteid='529dee04ec92e60208567ca6' limit 1000\"\"\"\n",
    "\n",
    "\n",
    "def read_hive(query):\n",
    "    conn = pyhs2.connect(host='example.com',\n",
    "                       port=10000,\n",
    "                       authMechanism=\"PLAIN\",\n",
    "                       user='XXXXX',\n",
    "                       password='XXXXX',\n",
    "                       database='default')\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(query)\n",
    "        #Return column info from query\n",
    "    if cur.getSchema() is None:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        return None\n",
    "\n",
    "    columnNames = [a['columnName'] for a in  cur.getSchema()] \n",
    "    print columnNames\n",
    "    columnNamesStrings = [a['columnName'] for a in  cur.getSchema() if a['type']=='STRING_TYPE'] \n",
    "    output =  pd.DataFrame(cur.fetch(),columns=columnNames)   \n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return output\n",
    "\n",
    "read_hive(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
